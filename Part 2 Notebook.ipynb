{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the CSV file into a DataFrame\n",
    "crsp_daily = pd.read_csv('./crsp_dret.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'ticker' and 'date'\n",
    "crsp_daily = crsp_daily.sort_values(by=['ticker', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ret           vol\n",
      "mean    5.402425e-04  1.364990e+06\n",
      "median  0.000000e+00  2.616000e+05\n",
      "std     5.333287e-02  5.526205e+06\n",
      "count   1.794757e+06  1.795289e+06\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "summary_stats = crsp_daily[['ret', 'vol']].describe()\n",
    "\n",
    "summary_stats = summary_stats.loc[['mean', '50%', 'std', 'count']]\n",
    "\n",
    "summary_stats.rename(index={'50%': 'median'}, inplace=True)\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp  users_holding ticker        date\n",
      "0       2018-05-03 14:45:59            587      A  2018-05-03\n",
      "1       2018-05-03 15:46:00            586      A  2018-05-03\n",
      "2       2018-05-03 16:46:00            587      A  2018-05-03\n",
      "3       2018-05-03 17:45:59            587      A  2018-05-03\n",
      "4       2018-05-05 14:45:59            588      A  2018-05-05\n",
      "...                     ...            ...    ...         ...\n",
      "7456372 2020-08-12 15:47:45           1475    OZK  2020-08-12\n",
      "7456373 2020-08-12 16:47:28           1477    OZK  2020-08-12\n",
      "7456374 2020-08-13 14:55:47           1480    OZK  2020-08-13\n",
      "7456375 2020-08-13 15:54:20           1482    OZK  2020-08-13\n",
      "7456376 2020-08-13 16:54:16           1482    OZK  2020-08-13\n",
      "\n",
      "[7456377 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('./robinhood_users_holding.csv')\n",
    "\n",
    "# Convert timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Create new column\n",
    "df['date'] = df['timestamp'].apply(lambda x: x.date().isoformat())\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticker        date  users_close\n",
      "0            A  2018-05-03          586\n",
      "1            A  2018-05-05          588\n",
      "2            A  2018-05-06          588\n",
      "3            A  2018-05-07          582\n",
      "4            A  2018-05-08          577\n",
      "...        ...         ...          ...\n",
      "1765425    OZK  2020-08-09         1491\n",
      "1765426    OZK  2020-08-10         1487\n",
      "1765427    OZK  2020-08-11         1480\n",
      "1765428    OZK  2020-08-12         1475\n",
      "1765429    OZK  2020-08-13         1482\n",
      "\n",
      "[1765430 rows x 3 columns]\n",
      "count    1.765430e+06\n",
      "mean     3.098802e+03\n",
      "std      2.118899e+04\n",
      "min      0.000000e+00\n",
      "25%      7.800000e+01\n",
      "50%      2.820000e+02\n",
      "75%      1.044000e+03\n",
      "max      9.449220e+05\n",
      "Name: users_close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Extract hour and minute for filtering\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['minute'] = df['timestamp'].dt.minute\n",
    "\n",
    "# I extracted the hour and minute for filtering to avoid complications mixing the 'datetime64' and 'time' types\n",
    "\n",
    "# Filter out entries from 4 PM or later\n",
    "df_before_4pm = df[(df['hour'] < 16)]\n",
    "\n",
    "# Group by 'ticker' and 'date'\n",
    "user_daily = df_before_4pm.groupby(['ticker', 'date']).last().reset_index()\n",
    "\n",
    "# Select the necessary columns and rename\n",
    "user_daily = user_daily[['ticker', 'date', 'users_holding']]\n",
    "user_daily.rename(columns={'users_holding': 'users_close'}, inplace=True)\n",
    "\n",
    "# Calculate summary statistics for 'users_close'\n",
    "summary_stats_users = user_daily['users_close'].describe()\n",
    "\n",
    "print(user_daily)\n",
    "print(summary_stats_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 objects freed from memory. Or you could install more RAM :)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del df\n",
    "collected = gc.collect()\n",
    "print(f\"{collected} objects freed from memory. Or you could install more RAM :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            userchg  abnormal_userchg  abnormal_userchg_lag\n",
      "count  1.762864e+06      1.752608e+06          1.750045e+06\n",
      "mean   1.016733e+01     -5.947388e-02         -5.409712e-02\n",
      "std    2.652290e+02      1.965703e+02          1.960411e+02\n",
      "min   -3.998100e+04     -4.259720e+04         -4.259720e+04\n",
      "25%   -1.000000e+00     -1.400000e+00         -1.400000e+00\n",
      "50%    0.000000e+00      0.000000e+00          0.000000e+00\n",
      "75%    1.000000e+00      1.200000e+00          1.200000e+00\n",
      "max    6.277900e+04      4.081540e+04          4.081540e+04\n"
     ]
    }
   ],
   "source": [
    "user_daily['date'] = pd.to_datetime(user_daily['date'])\n",
    "user_daily.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "# Calculate 'userchg'\n",
    "user_daily['userchg'] = user_daily.groupby('ticker')['users_close'].diff()\n",
    "\n",
    "# Calculate 'abnormal_userchg'\n",
    "rolling_mean = user_daily.groupby('ticker')['userchg'].transform(lambda x: x.rolling(window=5).mean())\n",
    "user_daily['abnormal_userchg'] = user_daily['userchg'] - rolling_mean\n",
    "\n",
    "# Calculate 'abnormal_userchg_lag'\n",
    "user_daily['abnormal_userchg_lag'] = user_daily.groupby('ticker')['abnormal_userchg'].shift(1)\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = user_daily[['userchg', 'abnormal_userchg', 'abnormal_userchg_lag']].describe()\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticker       date  users_close  userchg  abnormal_userchg  \\\n",
      "0            A 2018-05-03          586      NaN               NaN   \n",
      "1            A 2018-05-07          582     -6.0               NaN   \n",
      "2            A 2018-05-08          577     -5.0               NaN   \n",
      "3            A 2018-05-09          582      5.0               5.8   \n",
      "4            A 2018-05-10          584      2.0               2.8   \n",
      "...        ...        ...          ...      ...               ...   \n",
      "1197235    OZK 2020-08-07         1487     -2.0              -1.4   \n",
      "1197236    OZK 2020-08-10         1487     -4.0              -3.6   \n",
      "1197237    OZK 2020-08-11         1480     -7.0              -5.2   \n",
      "1197238    OZK 2020-08-12         1475     -5.0              -2.6   \n",
      "1197239    OZK 2020-08-13         1482      7.0               8.8   \n",
      "\n",
      "         abnormal_userchg_lag        vol       ret  \n",
      "0                         NaN  2365851.0  0.006524  \n",
      "1                         NaN  1468735.0  0.005821  \n",
      "2                         NaN  1916103.0 -0.000297  \n",
      "3                         NaN  2108506.0  0.011281  \n",
      "4                         5.8  1844868.0  0.010715  \n",
      "...                       ...        ...       ...  \n",
      "1197235                   0.2   667766.0  0.027530  \n",
      "1197236                   0.0  1078979.0  0.018519  \n",
      "1197237                  -3.6   917746.0  0.016248  \n",
      "1197238                  -5.2   482663.0 -0.008755  \n",
      "1197239                  -2.6   507562.0 -0.004224  \n",
      "\n",
      "[1197240 rows x 8 columns]\n",
      "        users_close       userchg  abnormal_userchg  abnormal_userchg_lag  \\\n",
      "count  1.197240e+06  1.194771e+06      1.189400e+06          1.187032e+06   \n",
      "mean   3.187587e+03  1.345424e+01      2.773944e+00          1.074463e+00   \n",
      "std    2.157242e+04  3.110853e+02      2.151529e+02          2.027918e+02   \n",
      "min    0.000000e+00 -3.998100e+04     -4.259720e+04         -2.891280e+04   \n",
      "25%    8.600000e+01 -2.000000e+00     -1.400000e+00         -1.400000e+00   \n",
      "50%    2.960000e+02  0.000000e+00      0.000000e+00          0.000000e+00   \n",
      "75%    1.084000e+03  2.000000e+00      1.400000e+00          1.200000e+00   \n",
      "max    9.449220e+05  6.277900e+04      4.081540e+04          4.081540e+04   \n",
      "\n",
      "                vol           ret  \n",
      "count  1.197081e+06  1.196867e+06  \n",
      "mean   1.368044e+06  5.973104e-04  \n",
      "std    5.413138e+06  5.104528e-02  \n",
      "min    0.000000e+00 -8.045770e-01  \n",
      "25%    6.320100e+04 -1.452200e-02  \n",
      "50%    2.789410e+05  0.000000e+00  \n",
      "75%    9.668020e+05  1.406400e-02  \n",
      "max    5.338918e+08  1.025182e+01  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'date' to datetime format in 'crsp_daily'\n",
    "crsp_daily['date'] = pd.to_datetime(crsp_daily['date'])\n",
    "\n",
    "# Merge the DataFrames on 'ticker' and 'date'\n",
    "user_merged = pd.merge(user_daily, crsp_daily, on=['ticker', 'date'], how='inner')\n",
    "\n",
    "# Sort the merged DataFrame by 'ticker' and 'date'\n",
    "user_merged.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "# Calculate summary statistics for all variables except 'ticker' and 'date'\n",
    "summary_stats_merged = user_merged.drop(columns=['ticker', 'date']).describe()\n",
    "\n",
    "print(user_merged)\n",
    "print(summary_stats_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              date ticker  abs_return    rank  extreme_absolute_return  \\\n",
      "0       2018-05-03      A    0.006524  1327.0                        0   \n",
      "1       2018-05-07      A    0.005821  1351.0                        0   \n",
      "2       2018-05-08      A    0.000297  1862.0                        0   \n",
      "3       2018-05-09      A    0.011281  1015.0                        0   \n",
      "4       2018-05-10      A    0.010715   929.0                        0   \n",
      "...            ...    ...         ...     ...                      ...   \n",
      "1197235 2020-08-07    OZK    0.027530   943.0                        0   \n",
      "1197236 2020-08-10    OZK    0.018519  1288.0                        0   \n",
      "1197237 2020-08-11    OZK    0.016248  1224.0                        0   \n",
      "1197238 2020-08-12    OZK    0.008755  1600.0                        0   \n",
      "1197239 2020-08-13    OZK    0.004224  1960.0                        0   \n",
      "\n",
      "         extreme_absolute_return_lag  \n",
      "0                                0.0  \n",
      "1                                0.0  \n",
      "2                                0.0  \n",
      "3                                0.0  \n",
      "4                                0.0  \n",
      "...                              ...  \n",
      "1197235                          0.0  \n",
      "1197236                          0.0  \n",
      "1197237                          0.0  \n",
      "1197238                          0.0  \n",
      "1197239                          0.0  \n",
      "\n",
      "[1197240 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute return\n",
    "user_merged['abs_return'] = user_merged['ret'].abs()\n",
    "\n",
    "# Rank the absolute returns\n",
    "user_merged['rank'] = user_merged.groupby('date')['abs_return'].rank(method='min', ascending=False)\n",
    "\n",
    "# Create 'extreme_absolute_return'\n",
    "user_merged['extreme_absolute_return'] = (user_merged['rank'] <= 20).astype(int)\n",
    "\n",
    "# Create the lagged 'extreme_absolute_return'\n",
    "user_merged['extreme_absolute_return_lag'] = user_merged.groupby('ticker')['extreme_absolute_return'].shift(1)\n",
    "\n",
    "# Fill NaN values with 0 in the lagged column\n",
    "user_merged['extreme_absolute_return_lag'] = user_merged['extreme_absolute_return_lag'].fillna(0)\n",
    "\n",
    "print(user_merged[['date', 'ticker', 'abs_return', 'rank', 'extreme_absolute_return', 'extreme_absolute_return_lag']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       abnormal_userchg   R-squared:                       0.025\n",
      "Model:                            OLS   Adj. R-squared:                  0.025\n",
      "Method:                 Least Squares   F-statistic:                     7764.\n",
      "Date:                Tue, 07 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        05:01:10   Log-Likelihood:            -8.0448e+06\n",
      "No. Observations:             1187032   AIC:                         1.609e+07\n",
      "Df Residuals:                 1187027   BIC:                         1.609e+07\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -0.9225      0.381     -2.423      0.015      -1.669      -0.176\n",
      "C(year)[T.2019]                 0.0865      0.478      0.181      0.856      -0.851       1.024\n",
      "C(year)[T.2020]                 7.0565      0.527     13.395      0.000       6.024       8.089\n",
      "extreme_absolute_return_lag   168.1117      2.062     81.532      0.000     164.070     172.153\n",
      "abnormal_userchg_lag            0.1467      0.001    152.566      0.000       0.145       0.149\n",
      "==============================================================================\n",
      "Omnibus:                  2775294.516   Durbin-Watson:                   1.934\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):    4010978176540.673\n",
      "Skew:                          21.348   Prob(JB):                         0.00\n",
      "Kurtosis:                    9008.227   Cond. No.                     2.15e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.15e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Extract the year from the date\n",
    "user_merged['year'] = pd.DatetimeIndex(user_merged['date']).year\n",
    "\n",
    "# Define the regression model\n",
    "formula = 'abnormal_userchg ~ extreme_absolute_return_lag + abnormal_userchg_lag + C(year)'\n",
    "\n",
    "# Fit the regression model\n",
    "model = smf.ols(formula, data=user_merged).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186896, 13)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'ret' or 'abnormal_userchg_lag' are NaN\n",
    "user_merged = user_merged.dropna(subset=['ret', 'abnormal_userchg_lag'])\n",
    "\n",
    "print(user_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  decile  avg_return\n",
      "0    2018-05-10       1    0.009156\n",
      "1    2018-05-10       2    0.006445\n",
      "2    2018-05-10       3    0.005149\n",
      "3    2018-05-10       4    0.004445\n",
      "4    2018-05-10       5    0.003009\n",
      "...         ...     ...         ...\n",
      "5340 2020-08-13       6   -0.003980\n",
      "5341 2020-08-13       7    0.000780\n",
      "5342 2020-08-13       8   -0.003462\n",
      "5343 2020-08-13       9    0.010984\n",
      "5344 2020-08-13      10   -0.004436\n",
      "\n",
      "[5345 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "user_merged = user_merged.copy()\n",
    "\n",
    "# Assign deciles to 'abnormal_userchg_lag' for each date\n",
    "# Create a new column 'decile'\n",
    "user_merged['decile'] = user_merged.groupby('date')['abnormal_userchg_lag'].transform(\n",
    "    lambda x: pd.qcut(x, 10, labels=False, duplicates='drop') + 1\n",
    ")\n",
    "\n",
    "# Calculate the average return\n",
    "decile_returns = user_merged.groupby(['date', 'decile'])['ret'].mean().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "decile_returns.rename(columns={'ret': 'avg_return'}, inplace=True)\n",
    "\n",
    "print(decile_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            decile_1  decile_2  decile_3  decile_4  decile_5  decile_6  \\\n",
      "date                                                                     \n",
      "2018-05-10  0.009156  0.006445  0.005149  0.004445  0.003009  0.001544   \n",
      "2018-05-11  0.004360  0.003741  0.003064  0.002325  0.002664  0.002180   \n",
      "2018-05-14  0.000702  0.002271 -0.001826 -0.003615 -0.004997 -0.003564   \n",
      "2018-05-15  0.002087  0.000403  0.000503  0.001802  0.001820 -0.001974   \n",
      "2018-05-16  0.011500  0.008576  0.008498  0.008650  0.006110  0.007723   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2020-08-07  0.007975  0.013529  0.014119  0.017976  0.015365  0.016044   \n",
      "2020-08-10  0.005981  0.011519  0.008043  0.014146  0.010728  0.010953   \n",
      "2020-08-11 -0.024632 -0.009158  0.004408  0.000815  0.001201  0.003247   \n",
      "2020-08-12  0.010171  0.002111 -0.001896  0.005662  0.003826  0.006810   \n",
      "2020-08-13  0.006791  0.001791  0.001583 -0.003183 -0.006434 -0.003980   \n",
      "\n",
      "            decile_7  decile_8  decile_9  decile_10  decile_10_minus_1  \n",
      "date                                                                    \n",
      "2018-05-10  0.003091  0.003239  0.006065  -0.000377          -0.009533  \n",
      "2018-05-11  0.002578  0.000015  0.007096   0.003242          -0.001119  \n",
      "2018-05-14  0.001501 -0.001011  0.007640   0.011490           0.010788  \n",
      "2018-05-15 -0.000604 -0.003349  0.000391  -0.001731          -0.003818  \n",
      "2018-05-16  0.009860  0.006810  0.009817   0.005323          -0.006177  \n",
      "...              ...       ...       ...        ...                ...  \n",
      "2020-08-07  0.019588  0.020261  0.004906  -0.010065          -0.018040  \n",
      "2020-08-10  0.013049  0.017870  0.017522   0.040075           0.034094  \n",
      "2020-08-11 -0.003868 -0.003371 -0.009557  -0.020955           0.003677  \n",
      "2020-08-12  0.004114  0.005569 -0.001568   0.000119          -0.010053  \n",
      "2020-08-13  0.000780 -0.003462  0.010984  -0.004436          -0.011228  \n",
      "\n",
      "[544 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Pivot the table to wide format\n",
    "wide_format = decile_returns.pivot(index='date', columns='decile', values='avg_return')\n",
    "\n",
    "# Rename columns\n",
    "wide_format.columns = [f'decile_{int(col)}' for col in wide_format.columns]\n",
    "\n",
    "# Create the long-short portfolio variable\n",
    "wide_format['decile_10_minus_1'] = wide_format['decile_10'] - wide_format['decile_1']\n",
    "\n",
    "print(wide_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      decile_10_minus_1   R-squared:                       0.051\n",
      "Model:                            OLS   Adj. R-squared:                  0.041\n",
      "Method:                 Least Squares   F-statistic:                     4.872\n",
      "Date:                Tue, 07 May 2024   Prob (F-statistic):           0.000237\n",
      "Time:                        05:01:10   Log-Likelihood:                 1207.5\n",
      "No. Observations:                 455   AIC:                            -2403.\n",
      "Df Residuals:                     449   BIC:                            -2378.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0003      0.001     -0.369      0.712      -0.002       0.001\n",
      "mktrf          0.1711      0.055      3.106      0.002       0.063       0.279\n",
      "smb            0.0243      0.122      0.199      0.842      -0.215       0.264\n",
      "rmw           -0.6914      0.204     -3.397      0.001      -1.091      -0.291\n",
      "cma            0.8341      0.264      3.157      0.002       0.315       1.353\n",
      "hml           -0.1889      0.108     -1.747      0.081      -0.402       0.024\n",
      "==============================================================================\n",
      "Omnibus:                      848.833   Durbin-Watson:                   1.901\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           779526.221\n",
      "Skew:                          11.887   Prob(JB):                         0.00\n",
      "Kurtosis:                     204.377   Cond. No.                         344.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Annualized Alpha: -0.0754374665782273\n"
     ]
    }
   ],
   "source": [
    "fama_french = pd.read_csv('./ff_daily.csv')\n",
    "fama_french['date'] = pd.to_datetime(fama_french['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Merge the decile data (from the previous step) with the Fama-French factors\n",
    "data_merged = pd.merge(wide_format, fama_french, on='date', how='inner')\n",
    "\n",
    "# Regression analysis\n",
    "if not data_merged.empty and data_merged[['mktrf', 'smb', 'rmw', 'cma', 'hml']].isnull().any().any():\n",
    "    print(\"Data contains NaNs in factor columns.\")\n",
    "else:\n",
    "    X = data_merged[['mktrf', 'smb', 'rmw', 'cma', 'hml']]\n",
    "    X = sm.add_constant(X)\n",
    "    y = data_merged['decile_10_minus_1']\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.OLS(y, X, missing='drop').fit()\n",
    "    print(model.summary())\n",
    "\n",
    "    # Calculate the annualized alpha\n",
    "    annualized_alpha = model.params['const'] * 252\n",
    "    print(f\"Annualized Alpha: {annualized_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decile_10_minus_1` portfolio has an annualized alpha of approximately -0.075, losing to the benchmark adjusted for the Fama-French five factors. The negative alpha suggests that the portfolio isn't providing sufficient returns to justify its risks, particularly in terms of profitability and investment choices. Specifically, its significant negative response to profitability (RMW) and positive response to investment (CMA) imply that it may be overweight in less profitable and higher investment companies, which could be dragging down its overall performance. Also, the insignificant coefficients for size (SMB) and value (HML) indicate that these factors do not effectively explain the portfolio's returns, pointing towards potential issues in stock selection and strategy execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
